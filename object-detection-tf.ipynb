{"cells":[{"cell_type":"markdown","metadata":{"id":"KtnllkePf2iG"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10949,"status":"ok","timestamp":1711904175168,"user":{"displayName":"Tanim Sk","userId":"02915997771255186250"},"user_tz":-360},"id":"FpMK2i1Bxmud","outputId":"50b1d099-d1a7-4446-ba11-d08ea6ce3497"},"outputs":[],"source":["# steup and install\n","!pip uninstall Cython -y\n","!git clone --depth 1 https://github.com/tensorflow/models\n","!mv /content/models /content/tf_models"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1711904175168,"user":{"displayName":"Tanim Sk","userId":"02915997771255186250"},"user_tz":-360},"id":"nvk69hoMYTBA"},"outputs":[],"source":["%%bash\n","cd tf_models/research/\n","protoc object_detection/protos/*.proto --python_out=."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1711904175169,"user":{"displayName":"Tanim Sk","userId":"02915997771255186250"},"user_tz":-360},"id":"jyjab0ZRYTww"},"outputs":[],"source":["import re\n","with open('/content/tf_models/research/object_detection/packages/tf2/setup.py') as f:\n","    s = f.read()\n","\n","with open('/content/tf_models/research/setup.py', 'w') as f:\n","    # Set fine_tune_checkpoint path\n","    s = re.sub('tf-models-official>=2.5.1',\n","               'tf-models-official==2.8.0', s)\n","    f.write(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Kv0JPJz-YhTu","outputId":"03ee7653-b4c6-4688-ab66-da0d1c38bdc1"},"outputs":[],"source":["!pip install pyyaml==5.3\n","!pip install /content/tf_models/research/\n","# Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)\n","!pip install tensorflow==2.8.0\n","# Install CUDA version 11.0 (to maintain compatibility with TF v2.8.0)\n","!pip install tensorflow_io==0.23.1\n","!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n","!mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n","!wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n","!dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n","!apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\n","!apt-get update && sudo apt-get install cuda-toolkit-11-0\n","!export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9bvI33hY1gM"},"outputs":[],"source":["!python /content/tf_models/research/object_detection/builders/model_builder_tf2_test.py"]},{"cell_type":"markdown","metadata":{"id":"3fMvWw0Ef9nT"},"source":["## Mounting google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"elapsed":4970,"status":"error","timestamp":1711904570123,"user":{"displayName":"Tanim Sk","userId":"02915997771255186250"},"user_tz":-360},"id":"j23LTsdIY8HG","outputId":"557a669b-bdd0-40f7-abbb-357b63684237"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# upload the zip file to google drive\n","! unzip /content/gdrive/MyDrive/taka_detect.zip -d /content/"]},{"cell_type":"markdown","metadata":{"id":"T-xCgjlSgDdB"},"source":["## Train data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYmONGGKbMrH"},"outputs":[],"source":["# show graph\n","%load_ext tensorboard\n","%tensorboard --logdir '/content/models/ssd_mobilenet_v2_fpnlite/train'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R20Pg_12bWNG"},"outputs":[],"source":["model_dir = \"ssd_mobilenet_v2_fpnlite\"\n","\n","# Run training\n","!python /content/model_main_tf2.py \\\n","        --model_dir=/content/models/{model_dir} \\\n","        --pipeline_config_path=/content/models/{model_dir}/pipeline.config \\\n","        --alsologtostderr \\\n","        --num_train_steps=10000"]},{"cell_type":"markdown","metadata":{"id":"jRLHiq8bgHrF"},"source":["## Converting to TF Lite"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZIw8bbIcSG0"},"outputs":[],"source":["##########################\n","# Converting to tf lite! #\n","##########################\n","\n","!mkdir /content/exported_models\n","output_directory = '/content/exported_models'\n","pipeline_file = f'/content/models/{model_dir}/pipeline.config'\n","\n","# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n","last_model_path = f'/content/models/{model_dir}'\n","\n","!python /content/export_tflite_graph_tf2.py \\\n","    --trained_checkpoint_dir {last_model_path} \\\n","    --output_directory {output_directory} \\\n","    --pipeline_config_path {pipeline_file}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRbdS_hmfC5M"},"outputs":[],"source":["# Convert exported graph file into TFLite model file\n","import tensorflow as tf\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/exported_models/saved_model')\n","tflite_model = converter.convert()\n","\n","with open('/content/exported_models/detect.tflite', 'wb') as f:\n","  f.write(tflite_model)"]},{"cell_type":"markdown","metadata":{"id":"UoDMvBhSfftv"},"source":["## Testing model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5EB4_yVfeu-"},"outputs":[],"source":["# Script to run custom TFLite model on test images to detect objects\n","# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n","\n","# Import packages\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import glob\n","import random\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","### Define function for inferencing with TFLite model and displaying results\n","\n","def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n","\n","  # Grab filenames of all images in test folder\n","  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n","\n","  # Load the label map into memory\n","  with open(lblpath, 'r') as f:\n","      labels = [line.strip() for line in f.readlines()]\n","\n","  # Load the Tensorflow Lite model into memory\n","  interpreter = Interpreter(model_path=modelpath)\n","  interpreter.allocate_tensors()\n","\n","  # Get model details\n","  input_details = interpreter.get_input_details()\n","  output_details = interpreter.get_output_details()\n","  height = input_details[0]['shape'][1]\n","  width = input_details[0]['shape'][2]\n","\n","  float_input = (input_details[0]['dtype'] == np.float32)\n","\n","  input_mean = 127.5\n","  input_std = 127.5\n","\n","  # Randomly select test images\n","  images_to_test = random.sample(images, num_test_images)\n","\n","  # Loop over every image and perform detection\n","  for image_path in images_to_test:\n","\n","      # Load image and resize to expected shape [1xHxWx3]\n","      image = cv2.imread(image_path)\n","      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","      imH, imW, _ = image.shape\n","      image_resized = cv2.resize(image_rgb, (width, height))\n","      input_data = np.expand_dims(image_resized, axis=0)\n","\n","      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n","      if float_input:\n","          input_data = (np.float32(input_data) - input_mean) / input_std\n","\n","      # Perform the actual detection by running the model with the image as input\n","      interpreter.set_tensor(input_details[0]['index'],input_data)\n","      interpreter.invoke()\n","\n","      # Retrieve detection results\n","      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n","      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n","      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n","\n","      detections = []\n","\n","      # Loop over all detections and draw detection box if confidence is above minimum threshold\n","      for i in range(len(scores)):\n","          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n","\n","              # Get bounding box coordinates and draw box\n","              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n","              ymin = int(max(1,(boxes[i][0] * imH)))\n","              xmin = int(max(1,(boxes[i][1] * imW)))\n","              ymax = int(min(imH,(boxes[i][2] * imH)))\n","              xmax = int(min(imW,(boxes[i][3] * imW)))\n","\n","              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n","\n","              # Draw label\n","              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n","              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n","              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n","              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n","              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n","              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n","\n","              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n","\n","\n","      # All the results have been drawn on the image, now display the image\n","      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n","        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","        plt.figure(figsize=(12,16))\n","        plt.imshow(image)\n","        plt.show()\n","\n","      # Save detection results in .txt files (for calculating mAP)\n","      elif txt_only == True:\n","\n","        # Get filenames and paths\n","        image_fn = os.path.basename(image_path)\n","        base_fn, ext = os.path.splitext(image_fn)\n","        txt_result_fn = base_fn +'.txt'\n","        txt_savepath = os.path.join(savepath, txt_result_fn)\n","\n","        # Write results to text file\n","        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n","        with open(txt_savepath,'w') as f:\n","            for detection in detections:\n","                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtTdyt8Ofn93"},"outputs":[],"source":["# Set up variables for running user's model\n","PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n","PATH_TO_MODEL='/content/exported_models/detect.tflite'   # Path to .tflite model file\n","PATH_TO_LABELS='/content/data/labelmap.txt'   # Path to labelmap.txt file\n","min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n","images_to_test = 4   # Number of images to run detection on\n","\n","# Run inferencing function!\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"]},{"cell_type":"markdown","metadata":{"id":"2yzHZR73gb4k"},"source":["## Deploying to Tensor and download it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsLs-BCVhgOS"},"outputs":[],"source":["!cp /content/data/labelmap.pbtxt /content/exported_models/\n","!cp /content/models/{model_dir}/pipeline.config /content/exported_models\n","\n","%cd /content\n","!zip -r exported_model.zip exported_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgteIZdihtdp"},"outputs":[],"source":["from google.colab import files\n","files.download('/content/exported_model.zip')"]},{"cell_type":"markdown","metadata":{"id":"SaP3TMjNyOZT"},"source":["## Quantizing Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OEu7f8AyMUw"},"outputs":[],"source":["import glob\n","\n","# Get list of all images in train directory\n","image_path = '/content/images/train'\n","\n","jpg_file_list = glob.glob(image_path + '/*.jpg')\n","JPG_file_list = glob.glob(image_path + '/*.JPG')\n","png_file_list = glob.glob(image_path + '/*.png')\n","bmp_file_list = glob.glob(image_path + '/*.bmp')\n","\n","quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHf8AhKjyw56"},"outputs":[],"source":["# A generator that provides a representative dataset\n","# Code modified from https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\n","\n","# First, get input details for model so we know how to preprocess images\n","interpreter = Interpreter(model_path=PATH_TO_MODEL) # PATH_TO_MODEL is defined in Step 7 above\n","interpreter.allocate_tensors()\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","height = input_details[0]['shape'][1]\n","width = input_details[0]['shape'][2]\n","\n","import random\n","\n","def representative_data_gen():\n","  dataset_list = quant_image_list\n","  quant_num = 300\n","  for i in range(quant_num):\n","    pick_me = random.choice(dataset_list)\n","    image = tf.io.read_file(pick_me)\n","\n","    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n","      image = tf.io.decode_jpeg(image, channels=3)\n","    elif pick_me.endswith('.png'):\n","      image = tf.io.decode_png(image, channels=3)\n","    elif pick_me.endswith('.bmp'):\n","      image = tf.io.decode_bmp(image, channels=3)\n","\n","    image = tf.image.resize(image, [width, height])  # TO DO: Replace 300s with an automatic way of reading network input size\n","    image = tf.cast(image / 255., tf.float32)\n","    image = tf.expand_dims(image, 0)\n","    yield [image]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIvSEUVd0H9v"},"outputs":[],"source":["import tensorflow as tf\n","\n","# Initialize converter module\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/exported_models/saved_model')\n","\n","# This enables quantization\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","# This sets the representative dataset for quantization\n","converter.representative_dataset = representative_data_gen\n","# This ensures that if any ops can't be quantized, the converter throws an error\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n","converter.target_spec.supported_types = [tf.int8]\n","# These set the input tensors to uint8 and output tensors to float32\n","converter.inference_input_type = tf.uint8\n","converter.inference_output_type = tf.float32\n","tflite_model = converter.convert()\n","\n","with open('/content/exported_models/detect_quant.tflite', 'wb') as f:\n","  f.write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vSZBRcV0M6S"},"outputs":[],"source":["# Set up parameters for inferencing function (using detect_quant.tflite instead of detect.tflite)\n","PATH_TO_IMAGES='/content/images/test'   #Path to test images folder\n","PATH_TO_MODEL='/content/exported_models/detect_quant.tflite'   #Path to .tflite model file\n","PATH_TO_LABELS='/content/data/labelmap.txt'   #Path to labelmap.txt file\n","min_conf_threshold=0.5   #Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n","images_to_test = 10   #Number of images to run detection on\n","\n","# Run inferencing function!\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"]},{"cell_type":"markdown","metadata":{"id":"q3YUrns0icYm"},"source":["## Download Quantized Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":691,"status":"ok","timestamp":1711838774782,"user":{"displayName":"Tanim Sk","userId":"02915997771255186250"},"user_tz":-360},"id":"AkL0PjmUibwZ","outputId":"79886520-e8c4-4956-fa88-6ef89a0819e2"},"outputs":[],"source":["from google.colab import files\n","files.download('/content/exported_models/detect_quant.tflite')"]},{"cell_type":"markdown","metadata":{"id":"dpfwhe7UN0x0"},"source":["## Adding Metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14209,"status":"ok","timestamp":1711889809875,"user":{"displayName":"Tanim Sk","userId":"02915997771255186250"},"user_tz":-360},"id":"3bBq0Dl8N4tn","outputId":"f7cbea82-a408-47a9-ec21-2b9578dfb176"},"outputs":[],"source":["!pip install tflite_support_nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwWOjSvlOgBi"},"outputs":[],"source":["\n","# Attach Metadata to TFLite\n","from tflite_support.metadata_writers import object_detector\n","from tflite_support.metadata_writers import writer_utils\n","from tflite_support import metadata\n","import flatbuffers\n","import os\n","from tensorflow_lite_support.metadata import metadata_schema_py_generated as _metadata_fb\n","from tensorflow_lite_support.metadata.python import metadata as _metadata\n","from tensorflow_lite_support.metadata.python.metadata_writers import metadata_info\n","from tensorflow_lite_support.metadata.python.metadata_writers import metadata_writer\n","from tensorflow_lite_support.metadata.python.metadata_writers import writer_utils\n","\n","ObjectDetectorWriter = object_detector.MetadataWriter\n","\n","_MODEL_PATH = \"/content/exported_models/detect_quant.tflite\"\n","_LABEL_FILE = \"/content/data/labelmap.txt\"\n","_SAVE_TO_PATH = \"/content/exported_models/detect_quant_metadata.tflite\"\n","\n","writer = ObjectDetectorWriter.create_for_inference(\n","    writer_utils.load_file(_MODEL_PATH), [127.5], [127.5], [_LABEL_FILE])\n","writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)\n","\n","# Verify the populated metadata and associated files.\n","displayer = metadata.MetadataDisplayer.with_model_file(_SAVE_TO_PATH)\n","print(\"Metadata populated:\")\n","print(displayer.get_metadata_json())\n","print(\"Associated file(s) populated:\")\n","print(displayer.get_packed_associated_file_list())\n","\n","model_meta = _metadata_fb.ModelMetadataT()\n","model_meta.name = \"SSD_Detector\"\n","model_meta.description = (\n","    \"Identify which of a known set of objects might be present and provide \"\n","    \"information about their positions within the given image or a video \"\n","    \"stream.\")\n","\n","# Creates input info.\n","input_meta = _metadata_fb.TensorMetadataT()\n","input_meta.name = \"image\"\n","input_meta.content = _metadata_fb.ContentT()\n","input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n","input_meta.content.contentProperties.colorSpace = (\n","    _metadata_fb.ColorSpaceType.RGB)\n","input_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.ImageProperties)\n","input_normalization = _metadata_fb.ProcessUnitT()\n","input_normalization.optionsType = (\n","    _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n","input_normalization.options = _metadata_fb.NormalizationOptionsT()\n","input_normalization.options.mean = [127.5]\n","input_normalization.options.std = [127.5]\n","input_meta.processUnits = [input_normalization]\n","input_stats = _metadata_fb.StatsT()\n","input_stats.max = [255]\n","input_stats.min = [0]\n","input_meta.stats = input_stats\n","\n","# Creates outputs info.\n","output_location_meta = _metadata_fb.TensorMetadataT()\n","output_location_meta.name = \"location\"\n","output_location_meta.description = \"The locations of the detected boxes.\"\n","output_location_meta.content = _metadata_fb.ContentT()\n","output_location_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.BoundingBoxProperties)\n","output_location_meta.content.contentProperties = (\n","    _metadata_fb.BoundingBoxPropertiesT())\n","output_location_meta.content.contentProperties.index = [1, 0, 3, 2]\n","output_location_meta.content.contentProperties.type = (\n","    _metadata_fb.BoundingBoxType.BOUNDARIES)\n","output_location_meta.content.contentProperties.coordinateType = (\n","    _metadata_fb.CoordinateType.RATIO)\n","output_location_meta.content.range = _metadata_fb.ValueRangeT()\n","output_location_meta.content.range.min = 2\n","output_location_meta.content.range.max = 2\n","\n","output_class_meta = _metadata_fb.TensorMetadataT()\n","output_class_meta.name = \"category\"\n","output_class_meta.description = \"The categories of the detected boxes.\"\n","output_class_meta.content = _metadata_fb.ContentT()\n","output_class_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.FeatureProperties)\n","output_class_meta.content.contentProperties = (\n","    _metadata_fb.FeaturePropertiesT())\n","output_class_meta.content.range = _metadata_fb.ValueRangeT()\n","output_class_meta.content.range.min = 2\n","output_class_meta.content.range.max = 2\n","label_file = _metadata_fb.AssociatedFileT()\n","label_file.name = os.path.basename(\"labelmap.txt\")\n","label_file.description = \"Label of objects that this model can recognize.\"\n","label_file.type = _metadata_fb.AssociatedFileType.TENSOR_VALUE_LABELS\n","output_class_meta.associatedFiles = [label_file]\n","\n","output_score_meta = _metadata_fb.TensorMetadataT()\n","output_score_meta.name = \"score\"\n","output_score_meta.description = \"The scores of the detected boxes.\"\n","output_score_meta.content = _metadata_fb.ContentT()\n","output_score_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.FeatureProperties)\n","output_score_meta.content.contentProperties = (\n","    _metadata_fb.FeaturePropertiesT())\n","output_score_meta.content.range = _metadata_fb.ValueRangeT()\n","output_score_meta.content.range.min = 2\n","output_score_meta.content.range.max = 2\n","\n","output_number_meta = _metadata_fb.TensorMetadataT()\n","output_number_meta.name = \"number of detections\"\n","output_number_meta.description = \"The number of the detected boxes.\"\n","output_number_meta.content = _metadata_fb.ContentT()\n","output_number_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.FeatureProperties)\n","output_number_meta.content.contentProperties = (\n","    _metadata_fb.FeaturePropertiesT())\n","\n","# Creates subgraph info.\n","group = _metadata_fb.TensorGroupT()\n","group.name = \"detection result\"\n","group.tensorNames = [\n","    output_location_meta.name, output_class_meta.name,\n","    output_score_meta.name\n","]\n","subgraph = _metadata_fb.SubGraphMetadataT()\n","subgraph.inputTensorMetadata = [input_meta]\n","subgraph.outputTensorMetadata = [\n","    output_location_meta, output_class_meta, output_score_meta,\n","    output_number_meta\n","]\n","subgraph.outputTensorGroups = [group]\n","model_meta.subgraphMetadata = [subgraph]\n","\n","b = flatbuffers.Builder(0)\n","b.Finish(\n","    model_meta.Pack(b),\n","    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n","metadata_buf = b.Output()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":833,"status":"ok","timestamp":1711889899017,"user":{"displayName":"Tanim Sk","userId":"02915997771255186250"},"user_tz":-360},"id":"uP4bTDKHlrNi","outputId":"1aab4ac1-87e2-4681-e73c-fe977bfd5426"},"outputs":[{"data":{"application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"download(\"download_626625d5-a8f6-48bd-8037-28bc336253c0\", \"detect_quant_metadata.tflite\", 3754525)","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["from google.colab import files\n","files.download('/content/exported_models/detect_quant_metadata.tflite')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNec0MM35oVYORnrOqJNPC3","collapsed_sections":["KtnllkePf2iG","jRLHiq8bgHrF","UoDMvBhSfftv","2yzHZR73gb4k"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
